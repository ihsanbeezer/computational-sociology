---
title: "SOC 577 Homework 2: Collecting digital trace data"
author: "Your name here"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
# Do not edit this chunk

# The following lines define how the output of code chunks should behave
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(error = TRUE)

# Required packages
library(rmarkdown)
library(tidyverse)
library(knitr)
library(magrittr)
library(jsonlite)
library(rvest)
library(rtweet)
```

# Instructions

This assignment is designed to build your familiarity with API usage and web-scraping. As in the previous assignment, it will involve a combination of short written answers and coding in R. All answers should be written in this document. Please pay attention to any questions asked after completing a code chunk.

**Please begin by adding your name to the top of the document on line 3.**

### Requirements
You should be viewing this document in RStudio. If you have not done so already, make sure to install the required packages (see initial chunk). You can do this by clicking the ``Install`` button in the Packages tab in the lower-right corner of RStudio and following the directions on the installation menu. You can also install packages by entering ``install.packages(x)`` into the R Console, where ``x`` is the name of the package.

### Submitting the homework
Once you have finished the assignment please complete the following steps to submit it:

1. Click on the ``Knit`` menu at the top of the screen and select ``Knit to HTML``. This will execute the all of the code and render the RMarkdown document in HTML. Verify that this document contains all of your answers and that none of the chunks produce error messages.
2. Add this document *and* the HTML file to Github. Use ``Homework submitted`` as your main commit message.
3. Push the commit to Github.
4. Visit the Github repository in your browser and verify that the final version of both files has been correctly uploaded.

**Note carefully** This assignment will involve using an API and a web-scraper. When you ``Knit`` your document it will re-run all of your code to produce the HTML version. It is possible this may produce an error if you have somehow exceeded your API rate limit, for example by running your code shortly before using ``Knit``. Please pay attention to rate limits and wait if necessary to avoid such errors.

# **Part I: The Twitter API**

### Set-up
You will need to have an active API key for this part of the homework. If you already have a Twitter Developer account then you may use your existing key. Otherwise you will receive access via the Rutgers Organizational account.

Once you have access, you can then visit https://developer.twitter.com/en/portal/dashboard to set up an app. Please complete the following steps:

    1. Create a new Project. Give it an informative name (e.g. "Class project"), use case (e.g. "Use API for homework assignment") and description (e.g. "App used to complete a homework assignment").
    2. Once you have created a project, click the option to create an App.
    3. Choose a name for your app.
    4. Add a description if requested (can be the same as the Project)
    5. Scroll to `Authentication Settings` and click the option to enable `3-legged OAuth`.
    6. Copy http://127.0.0.1:1410 into the `CALLBACK URLS` box.
    7. Once you have created your app you will be able to see your API keys. Copy the API key and the API secret key.

Please paste your credentials into the `creds.json` template provided in this repository. **Do NOT** commit this file to Github when you submit the assignment.

We will be using the `rtweet` package to interact with the Twitter API. This package contains many different wrapper functions to make it easy to work with the API. You can find out more about the package on its Github repository: https://github.com/ropensci/rtweet

### Questions

Q1. Please add the name of your app to line 79 and run the following chunk to load your credentials and get the access token for the API. Like the Spotify example covered in lecture, this will automatically open up a browser window asking for your permission to authorize the app. Once you grant permission you will receive the following message: `Authentication complete. Please close this page and return to R.` 

If you get a `401` error it is likely because you did not follow steps 5 and 6 above. You can fix this by clicking the gear icon next to your app and editing the settings. Note that `eval=FALSE`. This is because you should only need to run this once. There is no need to run it again when you knit the document.

```{r q1, echo=TRUE, tidy=TRUE, eval=FALSE}
creds <- read_json("creds.json") # load credentials

# authenticate via web browser
token <- create_token( 
  app = "", ### Add your app name here
  consumer_key = creds$key,
  consumer_secret = creds$secret)
```

In one sentence, explain why you should keep your credentials in a separate file. Answer here:

Q2. You should now have authorization to use the Twitter API. When you run the chunk below you will see information about your app. You should see that the `<oauth_app>` name matches the name of your app.

If you use R again on this same computer you will not need to complete the entire authorization process in the previous chunk. You should be able to just load `rtweet` then run `get_token()`.

Modify the chunk options below to prevent the code *and* output of the chunk below from being displayed in the final file. The chunk should still be evaluated otherwise the document will fail to knit.
```{r q2, echo=TRUE, tidy=TRUE}
get_token()
```

Q3. Use the `search_tweets` function to find 1000 tweets using the hashtag #WinterStorm, ignoring any retweets. I encourage you to carefully read the help file for `search_tweets` to understand its functionality.
```{r q3, echo=TRUE, tidy=TRUE}
storm.tweets <- search_tweets() ### Complete the function arguments
```

Q4. Let's take a look at the data we just collected. The `rtweet` package automatically parses the JSON file returned by the Twitter API and covers the results into a tibble. Click on `storm.tweets` in the Environment window to open up the data Viewer and take a look at the information returned.

In the chunk below, complete the following operations

    a. Print the dimensions of the tibble.
    b. Print the text of the tweet that received the highest number of retweets.
    c. Order the table by the date and time tweets were created and print the most recent tweet.
    d. Find how many unique accounts tweeted using the hashtag and print the result.

```{r q4, echo=TRUE, tidy=TRUE}
# a

# b

# c

# d
```

Q5. Before collecting any more tweets you should check your rate limit. You can do this using the `rate_limits()` function in `rtweet`. You will notice that the resulting table is quite large, since there are different rate limits for different endpoints. These limits represent the number of times you can call a particular endpoint. 

Use the `filter` function to view the rows where you have used up some of your available API calls.

```{r q5, echo=TRUE, tidy=TRUE}
rl <- rate_limits()
rl %>% filter() ### Complete function argument
```

How many different endpoints have you used? 

How many tweets can you collect using the `search_tweets` function before you will reach your rate limit?

You can read more about how rate-limiting works here: https://developer.twitter.com/en/docs/twitter-api/rate-limits#v2-limits

Q6. Now we can move onto collecting some tweets from users' timelines. In the example below, we use `get_timelines` function to get the most recent 200 tweets published one the main accounts of the New York Times, Washington Post, and Wall Street journal.

Use `dplyr` to find:

    a. the mean number of retweets and favorites by publication
    b. the average ratio of favorites to retweets.

You should be able to complete both parts with a single pipe.
```{r q6, echo=TRUE, tidy=TRUE}
tl <- get_timeline(c("nytimes", "washingtonpost", "wsj"), n = 200)

tl %>% ### Your code here
```

Which publication received the highest number of retweets?

What is the maximum number of tweets we can receive from a timeline using the standard API?

Q7. Now that you have some experience using the API, we can start to put some different methods together.

    a. Using the results from Q3, find the 10 accounts with the highest number of favorites, considering all tweets in `storm.tweets` (e.g. sum favorites by account)
    b. Use `lookup_users` to retrieve profile information for each account, store this as `profile.info`.
    c. Use `get_timeline` to retrieve the most recent 100 tweets for each account, store this as `recent.tweets`.
    d. Iterate over the accounts. Each time print the screen name, bio, and their most favorited recent tweet, followed by a new line.

```{r q7, echo=TRUE, tidy=TRUE}
# a

# b

# c

# d
```

Q8. This final question is more open-ended and exploratory. It should serve as an opportunity for you to learn more about the Twitter API and `rtweet` and to consider how to use such data to conduct sociological analyses.

    a. Find a function in `rtweet` that we have not used so far (you may use multiple functions if desired, but at least one has to be new).
    b. Use the function to retrieve at least 500 results (e.g. 500 tweets, 500 user profiles). Store the results as `A`.
    c. Now use the function again but change the query, such that you get the same quantity but a different set of data. Store the results as `B`. 
    d. Concatenate the two sets of results into a single tibble, making sure to include a column indicating which set each row belongs to.
    e. Calculate a single metric of interest to compare A and B. Is there any noticeable difference?
    f. Use a t-test (or any other appropriate statistical test) to assess whether there is a statistically-significant difference between the two groups.
    g. In one or two sentences, describe the results of the analysis.

*Here's a simple example to make this more concrete. Let's say A is a collection of tweets with the word "dog" and B is a collection of tweets containing "cat". We might want to examine whether tweets in A tend to get more favorites than tweets in B. For (e), we could compare the mean number of favorites received by tweets in each set; for (f) we could then use a t-test to assess whether there is a difference between the means.*

```{r q8, echo=TRUE, tidy=TRUE}
### Your answer here
```

# **Part II: Web-scraping**

### Preparation
The second part of this assignment is designed to build familiarity with web-scraping. You will use `rvest` to collect data from Wikipedia. The lecture covered how to build a scraper and crawler from start-to-finish and how to package it up using functions. The goal of this part of the assignment is more modest. The aim is to demonstrate how you can extract and process different kinds of elements from a single web-page. Once you understand these fundamental aspects of web-scraping it is then relatively straightforward to use the same logic to paginate and scrape other content from the same web-page.

Before starting this section, I recommend you get some more familiarity with HTML and CSS. Here are a few resources that you should find useful:

  - HTML+CSS tutorial https://github.com/cassidoo/HTML-CSS-Tutorial 
  - A quick reference for CSS selectors https://www.w3schools.com/cssref/css_selectors.asp
  - Interactive CSS tutorial/game https://flukeout.github.io/
  - CSS selector gadget https://selectorgadget.com/

I did not mention it in lecture, but you can right click on source code when inspecting it in your browser and click Copy > CSS Selector to directly copy the relevant CSS selector to your clipboard. This is an imperfect approach and you may need to edit the copied result, but it can often get you something close to what you are looking for.

Q9. For each of the following HTML tags, write a one sentence description of what they do (e.g. `<head> ... </head>`: This defines the header of a web page):
    
    a. `<title> ... </title>`:
    b. `<p> ... </p>`:
    c. `<img src="...">`:
    d. `<a href="...">...</a>`

Q10. In this example will be scraping content from Wikipedia. 

In the chunk below, `rvest` is used to read a Wikipedia page on various historical revolutions, rebellions, insurrections and uprisings. I recommend you open this page in your browser and take a look at the content before proceeding.

Complete the code below to extract the two tables from the page and store them as separate objects. Note that there is a specific function in `rvest` which can help you to process the tables. I have provided the relevant CSS selector.

```{r q10, echo=TRUE, tidy=TRUE}
### Your code here
wiki <- read_html("https://en.wikipedia.org/wiki/List_of_revolutions_and_rebellions")

t <- wiki %>% html_nodes("table.wikitable") %>% ### Complete the pipe

class(t) # It will help to know what class t is

BC <- # Extract table for BC from results and convert to tibble
AD <- # Extract table for AD 1-999 from results and convert to tibble
  
print(dim(BC)) # Do not modify these lines
print(dim(AD))
```

Q11. We can also extract other finds of elements from the HTML. Add the correct content to  retrieve the URL for the image of the storming of the Bastille. You will need to find the appropriate selector to pass to `html_nodes` and then the correct HTML attribute to select in `html_attr`. You do not need to add any additional functions to the pipe.

```{r q11, echo=TRUE, tidy=TRUE}
img.url <- wiki %>% 
  html_nodes() %>% # Add the relevant CSS selector
  html_attr() # Add the relevant HTML attribute name
```

When you render this document to HTML, the HTML snippet below will then use the `img.url` variable as the image source and will display the image in the output file.

<center><img src="`r img.url`"></center>

Q12. You should have noticed that all events after 1000 AD are listed as bullet points, rather than in tabular form. The HTML structure of these is called an unordered list. 

    a. Complete the first line of the chunk below to select all nodes of this type.
    If you inspect this object you should see that you have a character vector where each element is a string representing the contents of a bullet point. However, if you look at the tail of the vector you will see that there is also some other "junk" we are not interested in. Fortunately, it appears that most of the relevant elements contain a numeric year.
    b. Complete the regular expression in `str_extract` to detect whether there is a numeric year (e.g. 1848, 1968, 2011) in each element. Note: You do not need to detect all years, since some rows may have multiple years (e.g. 1501-1504), any reference is find.
    c. Combine `bullets` and `years` into a two column tibble then drop any rows where no year was detected. 
    d. Open up the tibble in the data viewer. You should see that there is still some unwanted information at the head and tail (e.g. rows where there is no event mentioned in the bullets column. Use tibble indexing to trim the dataset to remove these rows.

```{r q12, echo=TRUE, tidy=TRUE}
# a
bullets <- wiki %>% html_nodes() %>% html_text()

# b
years <- bullets %>% str_extract() %>% strtoi()

# c 
events <- 
  
# d
  
```

Fortunately for those of you interested in Wikipedia, there is a well-developed API and associated R packages (`WikipediaR` seems to be the most popular) so you can collect these data without web-scraping.

# End
Follow the submission instructions at the beginning of this document. The procedure is the same as for homework 1, but read the note on line 48 carefully.